# Methodologies

This section documents practical methods for using artificial intelligence in recruiting **without delegating judgment or accountability**.

The focus here is not on tools or vendors, but on *patterns of use*: how systems are tested, implemented, governed, and constrained in real hiring workflows.

Methods are organised by **problem type**, not by AI technique.

---

## Scope

The methodologies in this section aim to:
- make AI-assisted recruiting systems more inspectable
- reduce hidden automation and responsibility diffusion
- support human decision-making rather than replace it
- surface trade-offs and failure modes early

They do not assume that AI improves hiring outcomes by default.

---

## Structure

- **[Evaluation & testing](./evaluation-and-testing/)**  
  Methods for understanding how systems behave, change over time, and fail.

- **[Implementation patterns](./implementation-patterns/)**  
  Ways of introducing AI into workflows without making it decisive.

- **[Candidate-facing design](./candidate-facing-design/)**  
  How AI use is disclosed, explained, and contested by candidates.

- **[Governance & ownership](./governance-and-ownership/)**  
  Who is responsible when systems err, and how that responsibility is enforced.

- **[Failure modes](./failure-modes/)**  
  Common ways augmentation breaks down in practice.


Each subsection contains both stable guidance and links to related field notes.

---

## Status

This section is under active development.

Some methodologies are incomplete or exist only as outlines.  
Where uncertainty remains, it is documented explicitly rather than filled with best-practice claims.
