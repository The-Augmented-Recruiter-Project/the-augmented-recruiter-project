# Candidate Feedback Signals  
*(What candidates report, not what systems claim)*

This method focuses on collecting and interpreting **candidate-reported signals** about AI-mediated hiring experiences.

It treats feedback as a **diagnostic input**, not a performance score.

---

## What this method asks

Rather than “Are candidates happy?”, this method asks:

- Did candidates understand what was happening?
- Did they feel informed or ignored?
- Did automation increase or reduce clarity?
- Did the process feel respectful of their time?
- Did candidates believe a human was involved?

These questions often matter more than satisfaction ratings.

---

## Types of feedback signals

### Lightweight experience metrics
Examples include:
- eNPS-style questions
- short post-stage pulse surveys
- single-question check-ins

Used sparingly, these can surface trend changes over time.

---

### Qualitative prompts (often more useful)
Examples:
- “What confused you most in this process?”
- “At what point did you feel least informed?”
- “Did anything feel automated in a way you didn’t expect?”

Small volumes of honest responses often outweigh large scores.

---

### Behavioural feedback
Signals such as:
- drop-off after automated messages
- candidates disengaging mid-process
- candidates reapplying or not returning

These often speak louder than survey data.

---

## Using eNPS and benchmarks carefully

Metrics like eNPS can be useful **only when**:
- tracked over time
- interpreted directionally
- combined with qualitative signals

They should not be:
- used as success claims
- compared across organisations
- tied directly to system performance

---

## What this method does *not* do

- It does not prove candidate fairness
- It does not validate AI quality
- It does not substitute for accountability

Feedback is **contextual**, not definitive.

---

## Why this matters

Candidates experience automation long before organisations measure it.

Ignoring candidate feedback leads to:
- silent disengagement
- reputational damage
- loss of trust that metrics never capture

---

## Relationship to other methods

Candidate feedback signals connect:
- contextual interaction testing (micro experience)
- funnel change analysis (aggregate behaviour)
- auditability (answering “what happened?” later)

Together, they help keep human impact visible.

---

## Contribution notes

In-scope contributions include:
- anonymised feedback patterns
- changes observed after introducing automation
- cases where feedback was ignored with consequences

Promotional use of feedback metrics is out of scope.
