# Simple Drift Detection  
*(A recruiter-run “re-run and compare” test)*

This is a lightweight method for detecting whether a matching, search, or screening system has **changed behaviour over time**, even when nothing obvious has changed in the hiring setup.

It is designed to be run by recruiters without analytics support or specialised tooling.

---

## What this test does

This test helps answer one question:

> *“If I run the same inputs again later, do I get meaningfully different results?”*

It does not explain why change occurred — only that it did.

---

## What you need

- A small, fixed **candidate × role set**  
  (e.g. 15–30 CVs and 1–2 role definitions)

- Access to the same system you normally use  
  (ATS, matching tool, search interface)

- A simple way to capture results  
  (screenshots, export, notes)

---

## How to run the test

1. **Run a baseline**  
   Run your usual matching or search process using the fixed inputs.  
   Capture the top and bottom results.

2. **Wait**  
   Do not intentionally change the role, candidate set, or configuration.  
   Two to four weeks is usually enough.

3. **Re-run**  
   Run the same process again with the same inputs.

4. **Compare**  
   Look for noticeable changes rather than exact scores.

---

## What to look for

Drift may be present if you notice:
- candidates moving up or down unexpectedly
- candidates appearing or disappearing
- internal or known profiles behaving differently
- edge cases crossing cut-offs
- changes without a stated system update

You do not need to quantify the change for it to matter.

---

## What this test does *not* tell you

- It does not explain the cause of drift  
- It does not measure fairness or accuracy  
- It does not generalise beyond the test context  

Treat it as a **smoke alarm**, not a diagnosis.

---

## Why this matters

Most recruiting systems change gradually and quietly.

This test makes change visible at the point where decisions are actually influenced — without requiring trust in dashboards or vendor metrics.

---

## Relationship to other methods

This test works best alongside:
- shared candidate × role set testing
- desk-side or coffee shop review
- longer-term funnel analysis

---

## Contribution notes

This method is open to contribution.

Short write-ups of:
- when drift was detected
- what changed
- how teams responded (or didn’t)

are all in scope, provided they avoid promotional framing.
