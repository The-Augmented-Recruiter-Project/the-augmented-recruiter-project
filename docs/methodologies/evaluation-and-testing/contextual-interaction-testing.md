# Contextual Interaction Testing  
*(Informally: “coffee shop tests”)*

Contextual interaction testing evaluates how people experience, interpret, and respond to AI-mediated interactions in recruiting and hiring processes.

These tests focus on **human understanding and behaviour**, not model performance.

They are typically conducted in low-ceremony, informal settings and are designed to surface confusion, friction, misplaced trust, or unintended influence early.

---

## What this method tests

Contextual interaction testing is concerned with questions such as:

- Do people understand what this interaction is asking them to do?
- What do they believe is happening behind the scenes?
- Who do they think is making decisions?
- How does this interaction make them feel?
- What action do they actually take?

The answers to these questions often differ from system intent.

---

## Who this testing is for

This method can be used with:
- candidates
- hiring managers
- recruiters
- internal stakeholders interacting with AI-assisted tools

It is especially useful where AI appears in:
- application forms
- automated reminders or messages
- chatbots or conversational interfaces
- screening or scheduling flows
- status updates and rejection communications

---

## Typical test format

Tests are intentionally simple and conversational.

Examples of prompts include:

### Comprehension and expectation
- “Can you interact with this form?”
- “What do you think this message is asking you to do?”
- “What do you expect will happen next?”

### Emotional and relational response
- “How does this interaction feel?”
- “Do you like talking to this system?”
- “Does this feel helpful, annoying, or neutral?”

### Behavioural pathways
- “Which button would you press here?”
- “What do you ignore?”
- “Where do your eyes go first?”
- “At what point would you stop?”

### Mental models and responsibility
- “Who do you think is making decisions here?”
- “Do you think a human will see this?”
- “What would you do if something went wrong?”

Participants are not corrected during the test.

---

## Why informal settings matter

These tests are often done:
- at a desk
- in a coffee shop
- during a casual review session
- without scripts or incentives

This is deliberate.

Informal conditions:
- reduce performance behaviour
- surface genuine assumptions
- reveal how people behave when no one is watching

The goal is not polish, but honesty.

---

## What this method reveals

Contextual interaction testing frequently surfaces:
- false beliefs about human oversight
- over-attribution of intelligence or authority
- confusion caused by friendly or anthropomorphic language
- default and button bias
- silent self-exclusion by candidates
- trust breakdowns that analytics never capture

These are interaction failures, not UX bugs.

---

## What this method does *not* claim

Contextual interaction testing:
- is not statistically representative
- does not measure fairness
- does not validate models
- does not generalise across populations

It should be treated as an **early warning signal**, not a verdict.

---

## Relationship to other evaluation methods

This method complements:
- controlled input testing (e.g. shared CV sets)
- funnel and time-based analysis
- desk-side recruiter review

It is most effective when findings are used to inform:
- design changes
- disclosure language
- escalation paths
- human review points

---

## Contribution notes

This method is explicitly open to contribution.

Submissions may include:
- anonymised test prompts
- observed behaviours
- surprising responses
- design changes made as a result
- cases where findings were ignored and caused harm later

Vendor, in-house, and candidate-led perspectives are all in scope, provided they avoid promotional framing and make limitations explicit.
