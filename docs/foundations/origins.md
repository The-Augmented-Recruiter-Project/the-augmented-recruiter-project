# Origin and Context

This project did not emerge from theory or experimentation in isolation.

It is shaped by practical experience working with recruiting systems in Europe, where hiring is treated as a regulated, accountable, and socially consequential activity.

---

## Practical Constraints, Not Ideology

Recruiting in Europe operates under explicit constraints:

- personal data is protected by default  
- decision-making must be explainable  
- responsibility cannot be delegated to systems  
- employers are accountable for outcomes  

These are not abstract principles.  
They are enforced through regulation, labour law, and social expectation.

Working within these constraints makes certain design choices unavoidable:

- opacity is a liability, not a feature  
- automation without accountability is unacceptable  
- systems that obscure responsibility fail under scrutiny  

---

## Experience With Recruiting Systems

This project is informed by hands-on exposure to:

- applicant tracking systems  
- hiring workflows across technical and non-technical roles  
- interview processes involving multiple stakeholders  
- AI-assisted tooling introduced to “speed up” or “standardise” hiring  

A recurring pattern appears:

AI is asked to compensate for unclear roles, weak evaluation criteria, or distributed ownership of decisions.

When that happens, AI does not improve recruiting.  
It amplifies existing ambiguity and shifts risk onto candidates.

---

## A Response to Vendor-Led Narratives

Much of the current discourse around AI in recruiting is shaped by vendors.

This tends to prioritise:

- efficiency over accountability  
- scale over clarity  
- prediction over judgment  

This project does not reject technology.  
It rejects the framing of recruiting problems as primarily technical.

Recruiting failures are usually failures of definition, ownership, and intent.  
AI cannot resolve those failures and should not be used to mask them.

---

## Why Restraint Is a Design Principle

Recruiting decisions affect livelihoods, careers, and access to opportunity.

Once automated systems are embedded into hiring pipelines, they are difficult to audit, contest, or reverse.  
Errors compound quietly.

For that reason, restraint is treated here as a design requirement, not a limitation.

Augmentation is valuable when it:

- reduces friction  
- improves clarity  
- preserves human accountability  

Beyond that boundary, risk increases faster than benefit.

---

## Scope and Applicability

This project reflects European legal and cultural constraints, but it is not limited to Europe.

The principles outlined here are intended to be:

- portable across jurisdictions  
- adaptable to different organisational sizes  
- applicable regardless of tooling choices  

Where local laws are weaker, the arguments for transparency and accountability become stronger, not weaker.

---

## Traceability

This project was initiated by a single contributor working with recruiting systems in Europe.

Professional context is provided for traceability only:
- LinkedIn: https://www.linkedin.com/in/yourprofile

This reference is not intended to confer authority or ownership.  
The project does not depend on the author’s role, credentials, or ongoing involvement.

---

## Evolution

This project is published openly so that:

- assumptions can be challenged  
- failures can be documented  
- trade-offs can be made explicit  
- alternative models can be explored  

The intent is not consensus, but clarity.

Contributors are encouraged to add context, critique, and counterexamples, provided they preserve the project’s core commitments to transparency, accountability, and restraint.
