# Candidate Perspective

**This section is written for candidates.**

Specifically: job seekers who are trying to understand how AI and automation are used in recruiting, what their rights are, and what they can realistically do if they believe a hiring system has misjudged them.

This is not a guide for HR teams or vendors.  
It is not advice on how to “beat” hiring systems.

It exists to help candidates make sense of systems they are subjected to, but do not control.

---

## Why this section exists

Modern recruiting systems are usually designed, evaluated, and purchased without meaningful input from candidates.

As a result:
- automation is often invisible
- explanations are partial or misleading
- responsibility is unclear when systems fail
- candidates are expected to adapt without recourse

When something goes wrong, candidates are frequently told — implicitly or explicitly — that “the AI decided”, even when that is not an accurate description of what happened.

This section exists to:
- explain what is *actually* happening in recruiting systems
- correct common myths about “AI rejection”
- clarify what the law does and does not protect
- provide practical resources candidates can use if they want to question or respond

---

## A critical starting point: most rejections are not AI decisions

Many candidates assume that an automated rejection means:

> “An AI decided I wasn’t suitable.”

In most cases, this is not true.

What usually drives early rejection is a combination of:
- rule-based automation
- knock-out questions
- keyword or field filtering
- ranking or sorting logic
- human disengagement or overload

AI models may be involved in parts of the process — but far less often, and far less decisively, than marketing language suggests.

This distinction matters because:
- different systems imply different legal obligations
- different failures imply different remedies
- blaming “AI” often obscures human and organisational responsibility

Understanding what kind of system you are dealing with is the first step toward any meaningful response.

---

## What is commonly automated in recruiting

### Commonly used
- CV parsing and data extraction
- Knock-out questions (location, salary, visa status, availability)
- Keyword or field-based filtering
- Ranking or sorting candidate lists
- Automated rejection messaging

### Sometimes used
- Resume similarity or matching models
- Automated shortlisting support
- Chatbots for scheduling or FAQs

### Rarely used (despite assumptions)
- Fully autonomous hiring decisions
- AI systems with legal decision-making authority
- Self-directed systems operating without human configuration

Even when AI is used, a **human or organisation remains accountable** for the outcome.

---

## Why companies over-attribute decisions to “AI”

Invoking AI often:
- creates a sense of inevitability
- discourages questioning
- deflects responsibility
- reframes poor process design as technical necessity

This can leave candidates feeling that:
- nothing can be challenged
- explanations are meaningless
- outcomes are final and unreviewable

One purpose of this section is to make those dynamics visible.

---

## If you think an AI made a hiring decision about you

You are allowed to ask.

What matters is:
- how the question is framed
- what information you request
- what obligations the organisation actually has

This project provides:
- plain-language explanations of candidate rights
- examples of reasonable questions
- templates designed to avoid sounding adversarial or naive
- guidance on when organisations are required to respond

**See:**  
[What should I do if I think a hiring decision was made by AI?](/ai-decision-questions.md)

---

## If you think your data has been misused or mishandled

Recruiting systems process:
- personal data
- inferred or derived data
- sometimes sensitive or proxy data

Candidates often struggle to understand:
- whether AI was involved
- what consent really means in hiring
- how long their data is retained
- what they can realistically request or challenge

This section explains:
- the difference between AI use and automated processing
- what rights typically apply under data protection law
- how to make proportionate, effective requests
- when escalation may or may not be worth pursuing

**See:**  
[What do I do if I think my data is at risk or has been misused?](/data-rights-for-candidates.md)

---

## Is it AI, automation, or just poor process?

Many candidate frustrations attributed to “AI” are actually caused by:
- default ATS configurations
- brittle automation rules
- scale-driven shortcuts
- lack of human review

Understanding this difference helps candidates:
- ask better questions
- avoid being misled by technical language
- focus attention on organisational choices rather than abstractions

**See:**  
[The reality of what happens in a recruiting process](/recruiting-reality-check.md)

---

## Templates and practical resources

This section includes copy-ready templates designed to:
- be legally accurate
- remain calm and non-confrontational
- maximise the likelihood of a response
- minimise risk to the candidate

Templates include:
- asking whether automated decision-making was used
- requesting clarification on screening or ranking
- exercising data access or deletion rights
- challenging misleading or vague disclosures

**See:**  
[Candidate templates and request examples](/candidate-templates.md)

---

## What this section will not promise

This section does not promise:
- fairness
- guaranteed explanations
- successful appeals
- the ability to opt out of hiring systems
- meaningful recourse in all cases

Recruiting is a power-imbalanced system.  
This section aims to provide **clarity, not comfort**.

---

## How to use this section

You can:
- read it end-to-end
- jump directly to the situation that applies to you
- copy and adapt templates
- share links with other candidates
- use it to pressure-test explanations you receive

No technical background is required.

---

## Open questions

- When does explanation meaningfully help candidates?
- How much recourse is realistic in high-volume hiring?
- Where should human judgment be mandatory rather than optional?
- What does meaningful transparency look like at scale?

These questions are left open intentionally.
